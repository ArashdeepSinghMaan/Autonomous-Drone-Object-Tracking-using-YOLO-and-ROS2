# ðŸ§  Autonomous Drone Object Tracking using YOLO and ROS2

**End-to-end project** for developing, simulating, and deploying an **autonomous drone** that uses **YOLO** for real-time object detection and **ROS2** for intelligent tracking and control.  

The system detects a target (e.g., *person, car, or custom object*) using a camera feed, estimates its position in the image frame, and commands the drone to follow the target while maintaining safe distance and altitude.

---

## ðŸš€ Project Overview

This project demonstrates the integration of:
- **Computer Vision (YOLOv8 / YOLO-NAS)** for real-time object detection  
- **ROS2** for perceptionâ€“control communication  
- **Gazebo Simulation** for validation without hardware  
- **Optional PX4 Integration** for real-world deployment  

The drone autonomously detects and tracks a chosen target using onboard inference and visual servoing control.

---

## ðŸŽ¯ Objectives

- Detect and classify objects using YOLO in real time.  
- Track the centroid of the target and compute velocity/position commands.  
- Command the drone to align and follow the moving target.  
- Maintain safety and stability in simulation (and later, hardware).  

---

## ðŸ§© System Architecture
```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ROS2 Network â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Camera Sim â”‚â”€â”€â”€â–¶â”‚ YOLO Node â”‚â”€â”€â” â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ detections â”‚
â”‚ â–¼ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Tracker â”‚ â”‚
â”‚ â”‚ + Control â”‚â”€â”€â”€â”€â–¶â”‚ /cmd_vel (simulation) â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”‚ â”‚
â”‚ (future) â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ MAVROS2 â”‚â”€â”€â”€â”€â–¶â”‚ PX4 /setpoints â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
---


## âš™ï¸ Project Stages

### **Stage A â€” Simulation without PX4**
Focus on:
- YOLO integration  
- Target detection & tracking  
- Visual servoing control loop  

Test using **Gazebo/Ignition** and a simple UAV model (e.g., `hector_quadrotor`).

### **Stage B â€” Integration with PX4**
Add:
- PX4 SITL for flight dynamics  
- MAVROS2 bridge for offboard control  
- Safety and mission logic  

Deploy on Jetson + Pixhawk hardware.

---

## ðŸ§° Tech Stack

| Component | Technology |
|------------|-------------|
| **Middleware** | ROS2 Jazzy |
| **Simulation** | Gazebo  |
| **Autopilot (optional)** | PX4 + MAVROS2 |
| **Detection** | YOLOv8 / YOLO-NAS (Ultralytics) |
| **Languages** | Python / C++ |
| **Hardware (optional)** | Jetson Nano  + Pixhawk |
| **Deployment** | Docker / Native ROS2 workspace |

---

## ðŸ—ï¸ Project Structure

```bash
autonomous_drone_yolo/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ yolo_detector/           # YOLO node (subscribes camera, publishes detections)
â”‚   â”œâ”€â”€ tracker_controller/      # Tracker + control node
â”‚   â”œâ”€â”€ simple_drone_sim/        # Gazebo model + velocity interface
â”‚   â””â”€â”€ common_interfaces/       # Custom ROS2 message types (if any)
â”œâ”€â”€ launch/
â”‚   â”œâ”€â”€ simulation.launch.py     # Launch YOLO + tracker + sim
â”‚   â”œâ”€â”€ yolo_only.launch.py
â”‚   â””â”€â”€ px4_integration.launch.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ yolo_params.yaml
â”‚   â”œâ”€â”€ controller_gains.yaml
â”‚   â””â”€â”€ sim_params.yaml
â”œâ”€â”€ worlds/
â”‚   â”œâ”€â”€ tracking_test.world
â”‚   â””â”€â”€ environment.sdf
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ drone_model/
â”‚   â””â”€â”€ target_object/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_yolo.sh
â”‚   â”œâ”€â”€ convert_to_trt.py
â”‚   â””â”€â”€ data_logger.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile (optional)
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```
## ðŸ“Š YOLOv8 Training Results

This section presents the results from training the **YOLOv8n** model on the custom *Mobile Robot Detection Dataset* created and annotated in [Roboflow](https://roboflow.com/).

---

### âš™ï¸ Training Configuration

| Parameter | Value |
|------------|--------|
| **Dataset Version** | MobileRobotV1 |
| **Model** | YOLOv8n (pretrained on COCO) |
| **Epochs** | 100 |
| **Image Size** | 640x640 |
| **Batch Size** | 16 |
| **Framework** | Ultralytics YOLOv8 |
| **Hardware** | NVIDIA RTX 3060 |
| **Classes** | 1 (`robot`) |

---

### ðŸ“ˆ Performance Metrics

| Metric | Value | Description |
|---------|--------|-------------|
| **Precision (P)** | 0.93 | Correct detections / All detections |
| **Recall (R)** | 0.91 | Detections / Total ground truths |
| **mAP@50** | 0.95 | Mean Average Precision at IoU = 0.5 |
| **mAP@50-95** | 0.87 | Mean Average Precision across thresholds |

> ðŸ“Œ *The model achieved high precision and recall, demonstrating strong localization and classification performance on the robot detection task.*

---

### ðŸ“Š Training Curves

Below are the training curves showing the evolution of losses and accuracy across 100 epochs.

<p align="center">
  <img src="runs/detect/train/results.png" width="700" alt="Training Results Graph">
</p>

- **Box Loss / Cls Loss / DFL Loss** â†’ decrease over epochs, indicating model convergence  
- **Precision / Recall / mAP** â†’ increase steadily and plateau near high values  

---

### ðŸ¤– Example Detection Outputs

Sample validation images with predicted bounding boxes around the mobile robot:

| Detection |
|-----------|
| <img src="runs/detect/train/val_batch0_pred.jpg" width="300"> |
| <img src="runs/detect/train/val_batch1_pred.jpg" width="300"> |

> âœ… The trained model correctly identifies the robot in varied orientations and distances, even under partial occlusion.

---

### ðŸ§® Inference Speed

| Device | Resolution | FPS | Comments |
|---------|-------------|-----|-----------|
| RTX 3060 (GPU) | 640x640 | ~55 FPS | Real-time capable |
| Jetson Orin Nano | 480x480 | ~23 FPS | Suitable for onboard inference |

---

### ðŸ“¦ Model Artifacts

| File | Description |
|------|--------------|
| `runs/detect/train/weights/best.pt` | Best performing YOLO model (used in ROS2 node) |
| `runs/detect/train/results.png` | Training curves (loss, precision, recall) |
| `val_batch*_pred.jpg` | Example detections from validation set |

---

### ðŸš€ Integration Summary

The trained YOLOv8 model is integrated into the ROS2 node:
- Subscribes to `/camera/image_raw`
- Publishes detections to `/yolo/detections`
- Visualized in `/yolo/image_marked`

Result:  
The droneâ€™s onboard vision system now detects and tracks the **mobile robot** for visual-following control.

---

## ðŸ§¾ References
- [Ultralytics YOLOv8 Docs](https://docs.ultralytics.com)
- [Roboflow Dataset Management](https://roboflow.com)
- [ROS2 Jazzy Documentation](https://docs.ros.org/en/jazzy)
